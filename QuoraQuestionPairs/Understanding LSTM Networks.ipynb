{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복적 신경 회로망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 단어에 대한 이해를 바탕으로 각 단어를 이해하게됩니다.  \n",
    "전통적인 신경 네트워크는이를 수행 할 수 없습니다.\n",
    "\n",
    "예를 들어 영화의 모든 지점에서 어떤 종류의 이벤트가 발생 하는지를 분류하고 싶다고 가정 해보십시오.  \n",
    "전통적인 뉴럴 네트워크가 어떻게 영화의 이전 사건에 대한 추론을 사용하여 나중에 알려줄 수 있는지는 불분명합니다.\n",
    "\n",
    "Recurrent Neural Networks은 문제를 해결합니다.  \n",
    "그들은 그 안에 루프가있는 네트워크이며 정보가 지속되도록합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 다이어그램에서 신경 네트워크의 덩어리 인 A, 몇 가지 입력 x를 살펴 봅니다.  \n",
    "t값 (ht) 루프를 사용하면 네트워크의 한 단계에서 다음 단계로 정보를 전달할 수 있습니다.  \n",
    "\n",
    "이러한 루프는 반복적 인 신경망을 신비하게 보입니다.  \n",
    "그러나, 조금 더 생각한다면, 그것들은 보통의 신경망과 완전히 다르지 않다는 것이 밝혀졌습니다.  \n",
    "반복적인 신경망은 동일한 네트워크의 여러 복사본으로 생각할 수 있으며 각각은 후임자에게 메시지를 전달합니다.  \n",
    "루프를 풀면 어떻게 될지 생각해보십시오.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 사슬같은 성격은 목록과 밀접하게 관련되어 있음을 보여줍니다.   \n",
    "이러한 데이터에 사용할 신경망의 natural 아키텍처입니다.  \n",
    "\n",
    "지난 몇 년 동안 음성 인식, 언어 모델링, 번역, 이미지 캡션 등 다양한 문제에 RNN을 적용하여 놀라운 성공을 거두었습니다.\n",
    "\n",
    "이러한 성공의 핵심은 \"LSTMs\"를 사용하는 것인데, 이것은 매우 특별한 종류의 반복적 인 신경망으로 많은 작업에서 표준 버전보다 훨씬 효과적입니다.  \n",
    "반복적 인 신경망을 기반으로 한 거의 모든 흥미 진진한 결과는 이들과 함께 달성됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장기 의존성 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 매력 중 하나는 이전 비디오 프레임을 사용하여 현재 프레임의 이해를 알리는 것과 같이  \n",
    "이전 정보를 현재 작업에 연결할 수 있다는 아이디어입니다.  \n",
    "\n",
    "현재 작업을 수행하기 위해 최근 정보 만 살펴 봐야합니다.  \n",
    "예를 들어, 이전 단어를 기반으로 다음 단어를 예측하려고 시도하는 언어 모델을 생각해보십시오. \n",
    "\"구름이 하늘에 있다\"라는 마지막 단어를 예측하려고 한다면 구름 다음의 단어는 하늘이 될 것입니다.   \n",
    "이러한 경우, 관련 정보와 필요로하는 장소 사이의 <b>간격이 작은 경우</b> RNN은 과거 정보를 사용하는 법을 배울 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 더 많은 상황이 필요한 경우도 있습니다.  \n",
    "\"나는 프랑스에서 자랐습니다 나는 프랑스 빵을 즐겨먹는다 ... 나는 유창한 프랑스어를 구사합니다 ......\"라는 문장이 길어 진다면    \n",
    "더 뒤에서 프랑스의 맥락. 관련 정보가 매우 커져야 뒤에 까지 프랑스를 잊어버리지 않고 배우는게 가능합니다.   \n",
    "하지만 그 격차가 커짐에 따라 RNN은 정보를 연결하는 법을 배울 수 없게됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이론적으로 RNN에 매개 변수를 선택적으로 추가 하므로써 이러한 \"장기 의존성\"을 처리 할 수 있습니다.  \n",
    "\n",
    "LSTM입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory Networks (일반적으로 \"LSTM\"이라고 함)는 특별한 종류의 RNN이며 장기 의존성을 학습 할 수 있습니다.  \n",
    "그들은 Hochreiter & Schmidhuber (1997)에 의해 소개되었으며 , 다음 작업에서 많은 사람들이 세련되고 대중화되었습니다.  \n",
    "이들은 다양한 문제에 대해 대단히 잘 작동하며 널리 사용되고 있습니다.\n",
    "\n",
    "LSTM은 장기 의존성 문제를 피하기 위해 명시 적으로 설계되었습니다.  \n",
    "인간이 오랜 기간 동안 정보를 기억하는 것은 실제 배우는 것이 아닌 기본 행동입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 반복적 인 신경망은 신경망의 반복적 인 모듈 체인의 형태를 갖는다.  \n",
    "표준 RNN에서이 반복 모듈은 단일 tanh Layer와 같은 매우 간단한 구조를 갖습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM도이 체인과 같은 구조를 가지고 있지만 반복 모듈은 다른 구조를 가지고 있습니다.  \n",
    "하나의 신경 네트워크 계층을 갖는 대신 모듈 안에는 네 가지가 있으며 아주 특별한 방식으로 상호 작용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무슨 일이 일어나고 있는지에 대해 걱정할 필요가 없습니다.  \n",
    "나중에 LSTM 다이어그램을 단계별로 살펴 보겠습니다.  \n",
    "지금은 우리가 사용할 표기법에 익숙해 지도록 노력합시다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 다이어그램에서 각 행은 한 노드의 출력에서 다른 노드의 입력까지 전체 벡터를 전달합니다.  \n",
    "핑크색 원은 벡터 가산과 같은 pointwise 연산을 나타내고, 노란색 상자는 신경 네트워크 계층을 학습 한 것입니다.  \n",
    "병합되는 줄은 연결을 나타내는 반면 줄 바꿈은 내용이 복사되고 다른 줄로 이동한다는 것을 나타냅니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM의 핵심 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM의 핵심은 Cell 상태이며 다이어그램의 상단을 가로 지르는 수평선입니다.  \n",
    "\n",
    "셀 상태는 일종의 컨베이어 벨트와 같습니다.   \n",
    "그것은 약간의 작은 선형 상호 작용과 함께 체인 전체를 똑바로 따라 실행됩니다.  \n",
    "정보가 변경되지 않고 그대로 전달되는 것은 매우 쉽습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM에는 게이트라고하는 구조로 신중하게 규제되는 셀 상태에 정보를 제거하거나 추가 할 수있는 기능이 있습니다.    \n",
    "\n",
    "Gate는 정보를 선택적으로 전달할 수있는 방법입니다.   \n",
    "그것들은 sigmoid 신경망 층과 점으로 된 곱셈 연산으로 구성됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid layer는 0과 1 사이의 숫자를 출력하여 각 구성 요소의 얼마만큼을 통과시켜야하는지 설명합니다.  \n",
    "값 0은 \"아무 것도 통과시키지 말 것\"을 의미하지만 값 1은 \"모든 것을 통과 시키자\"는 의미입니다.  \n",
    "\n",
    "LSTM에는 셀 상태를 보호하고 제어하기 위해 세 개의 게이트가 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단계별 LSTM Walk Through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM의 첫 번째 단계는 셀 상태에서 벗어날 정보를 결정하는 것입니다.  \n",
    "이 결정은 \"게이트 층을 잊어 버리세요\"라는 sigmoid layer 의해 이루어집니다.  \n",
    "ht-1,xt sigmoid layer를 통해서 Ct-1에 1이면 완전히 이것을 유지하고 0 이걸 완전히 없애라이다.\n",
    "\n",
    "이전 단어를 기반으로 다음 단어를 예측하려고 시도하는 언어 모델의 예제로 돌아가 보겠습니다.    \n",
    "이러한 문제에서, Cell 상태는 현재 주체의 성별을 포함 할 수 있으므로 올바른 대명사를 사용할 수 있습니다.    \n",
    "새로운 주제가 보일 때, 우리는 옛 주제의 성별을 잊어 버릴 수 있습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계는 우리가 셀 상태에 저장할 새로운 정보를 결정하는 것입니다.  \n",
    "여기에는 두 부분이 있습니다.  \n",
    "먼저, \"입력 게이트 레이어\"라고하는 sigmoid레이어가 업데이트 할 값을 결정합니다.  \n",
    "다음, tanh 계층은 새로운 후보 값 벡터 C̃t있습니다.  \n",
    "다음 단계에서는이 두 요소를 결합하여 상태에 대한 업데이트를 만듭니다.\n",
    "\n",
    "우리 언어 모델의 예제에서, 우리는 잊고있는 오래된 것을 대체하기 위해 새로운 주체의 성별을 Cell 상태에 추가하려고합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이전 셀 상태 인 Ct-1를 업데이트 할 차례입니다.  \n",
    "새로운 Cell 상태 Ct가 만들어지기 이전 단계는 모두 실제로 수행해야합니다.\n",
    "\n",
    "우리는 오래된 상태에 ft를 곱하여 이전에 잊어 버리기로 한것들을 잊어 버린다. \n",
    "\n",
    "그런 다음 우리는 추가 it * C̃t 이 값은 새로운 후보 값으로 각 상태 값을 업데이트하기로 결정한 정도에 따라 조정 된다.  \n",
    "\n",
    "언어 모델의 경우 이전 단계에서 결정한대로 이전 주제의 성별에 대한 정보를 삭제하고 새로운 정보를 추가합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 우리는 출력 할 내용을 결정해야합니다.    \n",
    "이 출력은 Cell 상태를 기반으로하지만 필터링 된 버전이됩니다.    \n",
    "먼저 우리가 출력 할 Cell 상태의 부분을 결정하는 simoid레이어를 실행합니다.    \n",
    "그런 다음 tanh를 통해 Cell 상태를 저장합니다.  \n",
    "sigmoid 게이트의 출력을 곱하면 우리가 결정한 부분만 출력합니다.  \n",
    "\n",
    "언어 모델 예제에서는 주제를 보았기 때문에 동사와 관련된 정보를 출력 할 수 있습니다.  \n",
    "예를 들어 피사체가 단수인지 복수인지를 출력 할 수 있으므로 다음에 오는 경우 동사가 어떤 형태로 결합되어야하는지 알 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 장시간 단기 기억에 변종"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 설명한 것은 꽤 일반적인 LSTM입니다.  \n",
    "그러나 모든 LSTM이 위와 동일하지는 않습니다.  \n",
    "사실, LSTM을 포함하는 거의 모든형태가 약간 다른 버전을 사용하는 것처럼 보입니다.  \n",
    "그 차이는 사소하지만 그 중 일부는 언급 할만한 가치가 있습니다.\n",
    "\n",
    "Gers & Schmidhuber (2000)에 의해 소개 된 인기있는 LSTM 변형 중 하나 는 \"peephole connections\"을 추가하는 것입니다.  \n",
    "이것은 Gate Layer가 Cell 상태를 보게한다는 의미입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 다이어그램은 뚫린 구멍을 모든 문에 추가하지만 많은 논문은 \"peephole connections\" 제공하고 다른 문항은 제공하지 않습니다.  \n",
    "\n",
    "또 다른 변형은 결합 된 forget 및 input 게이트를 사용하는 것입니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM에서 약간 더 극적인 변화는 Gated Recurrent Unit 또는 GRU 이다. Cho, et al. (2014) . \n",
    "또한 잊어 버린 게이트와 입력 게이트를 하나의 \"업데이트 게이트\"에 결합합니다.  \n",
    "또한 셀 상태와 숨겨진 상태를 병합하고 다른 변경 작업도 수행합니다.  \n",
    "결과로 생성 된 모델은 표준 LSTM 모델보다 간단하며 점차 대중화되고 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것들은 가장 주목할만한 LSTM 변형의 일부에 지나지 않습니다.  \n",
    "야오 (Yao) 등의 Depth Gated RNN과 같은 다른 많은 것들이 있습니다 . (2015)   \n",
    "Koutnik 등의 Clockwork RNNs와 같은 장기 의존성을 다루는 데는 완전히 다른 접근 방식이 있습니다. (2014) .  \n",
    "\n",
    "\n",
    "다음 중이 변종 중 가장 적합한 것은 어느 것입니까? 차이점이 중요합니까?   \n",
    "Greff, et al. (2015) 는 인기있는 변종을 잘 비교하여 모두 동일하다는 것을 알게되었습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서, 사람들이 RNN으로 달성 한 주목할만한 결과를 언급했습니다.   \n",
    "본질적으로 이들 모두는 LSTM을 사용하여 달성됩니다.  \n",
    "그들은 실제로 대부분의 작업을 위해 더 잘 작동합니다.\n",
    "\n",
    "LSTM은 RNN으로 달성 할 수있는 큰 발전이었습니다.  \n",
    "RNN의 모든 단계에서 정보를 수집하여 더 많은 정보를 수집하는 것이 좋습니다.  \n",
    "예를 들어 RNN을 사용하여 이미지를 설명하는 캡션을 만들면 이미지의 일부를 선택하여 출력하는 모든 단어를 볼 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
