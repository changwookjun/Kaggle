{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Amazing Power Of Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'word2vec'이다. Mikolov et al. Google에서 단어의 효율적인 벡터 표현  (및 해당 단어로 수행 할 수있는 작업)   논문    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[벡터 공간에서의 단어 표현의 효율적인 추정 - Mikolov et al. 2013 년](https://arxiv.org/pdf/1301.3781.pdf)  \n",
    "[단어와 구문의 분산 된 표현과 그 구성 - Mikolov et al. 2013 년](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)  \n",
    "[연속 된 공간 단어 표현에서의 언어 규칙 - Mikolov et al. 2013 년](http://www.aclweb.org/anthology/N13-1090)  \n",
    "[word2vec 매개 변수 학습 설명 - Rong 2014](https://arxiv.org/pdf/1411.2738v3.pdf)  \n",
    "[word2vec라고 설명했다 : Mikolov et al의 부정 샘플링 Word-Embedding Method 유도 - Goldberg and Levy 2014](https://arxiv.org/pdf/1402.3722v1.pdf)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 논문의 첫 번째 글 ( '효율적인 추정 ...')에서 우리는 단어 벡터를 배우기위한 Continuous Bag-of-Words 및 Continued Skip-gram 모델에 대한 설명을 얻습니다 .  \n",
    "\n",
    "두 번째 논문에서 우리는 단어 벡터의 힘, 건너 뛰기 그램 모델 (계층 적 softmax 및 음수 샘플링)에 대한 최적화에 대한 추가 정보, 단어에 벡터를 적용하는 방법에 대한 설명을 더 많이 제공합니다.  \n",
    "\n",
    "세 번째 논문 ( 'Linguistic Regularities ...')은 단어 벡터를 기반으로 한 벡터 지향 추론을 기술하고 유명한 \"King - Man + Woman = Queen\"예제를 소개합니다.   \n",
    "\n",
    "마지막 두 논문은 Milokov 논문에서 매우 간결하게 표현 된 몇 가지 아이디어에 대한보다 자세한 설명을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Code 에서 [word2vec 구현](https://code.google.com/archive/p/word2vec/) 을 확인하십시오 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 벡터 란 무엇입니까?  \n",
    "한 수준에서, 그것은 단순히 가중치 벡터입니다.  \n",
    "간단한 1-of-N (또는 'one-hot') 인코딩에서 벡터의 모든 요소는 어휘의 단어와 연결됩니다.  \n",
    "주어진 단어의 인코딩은 단순히 해당 요소가 1로 설정되고 다른 모든 요소는 0 인 벡터입니다.  \n",
    "\n",
    "우리의 어휘에는 왕, 여왕, 남자, 여자, 그리고 어린이라는 단어가 5 개만 있다고 가정 해보십시오.  \n",
    "'Queen'이라는 단어를 다음과 같이 인코딩 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 인코딩을 사용하면 단어 벡터간에 의미있는 비교를 할 수 없습니다.  \n",
    "\n",
    "word2vec에서는 단어 의 분산 표현이 사용됩니다.  \n",
    "수백 개의 벡터를 가져옵니다.  \n",
    "각 단어는 해당 요소에 대한 가중치의 분포로 표현됩니다.  \n",
    "따라서 벡터의 요소와 단어 사이의 일대일 매핑 대신 단어의 표현이 벡터의 모든 요소에 걸쳐 적용되며 벡터의 각 요소는 여러 단어의 정의에 기여합니다.  \n",
    "\n",
    "가상의 단어 벡터에서 치수에 레이블을 지정하면 (물론 알고리즘에 사전 할당 된 레이블이 없음) 다음과 같이 보일 수 있습니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러한 벡터는 어떤 추상적인 방법으로 한 단어의 '의미'를 나타냅니다.  \n",
    "그리고 우리가 다음에 보게 될 것입니다.  \n",
    "단순히 큰 코퍼스를 검사함으로써 놀랍게도 표현적인 방식으로 단어 사이의 관계를 포착 할 수있는 단어 벡터를 배울 수 있습니다.  \n",
    "벡터를 신경망의 입력으로 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 벡터 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터는 a가 b와 같은 형태의 유추 질문에 대답하는 데 아주 능숙합니다.  \n",
    "예를 들어, 삼촌을 찾는다면 남자는 여자 거리 만큼(cosine distance)을 기반한 간단한 벡터 오프셋 (offset) 방법을 사용하여 이모를 만날수 있다.  \n",
    "다음은 성별 관계를 설명하는 세 단어 쌍에 대한 벡터 오프셋입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 여기에서 우리는 단수의 복수 관계를 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 종류의 벡터 구성으로 \"King - Man + Woman =?\"질문에 답하고 결과 \"Queen\"에 도착할 수 있습니다.  \n",
    "이 모든 지식은 단순히 의미에 대해 제공되는 다른 정보가없는 문맥에서 많은 단어를 보는 것으로부터 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왕, 남자, 여왕, 여자를 벡터로 표현하면 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 구성의 결과 King - Man + Woman =?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 동일한 기술을 사용하여 얻은 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 2 차원 PCA 예측에서 국가 - 수도 관계가 어떻게 생겼는지 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 'a is b to c is to be?'의 몇 가지 예입니다.   \n",
    "단어 질문에 의해 답변되는 스타일 질문 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 또한 '독일 + 항공사'와 같은 질문을하기 위해 요소를 추가하여 벡터 요소를 추가 할 수 있으며  \n",
    "복합 벡터에 가장 가까운 토큰을 보면 인상적인 답변을 찾을 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 의미 론적 관계가있는 단어 벡터는 기계 번역, 정보 검색 및 질문 응답 시스템과 같은 기존의 많은 NLP 응용 프로그램을 개선하는 데 사용될 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic-Syntatic 단어 관계는 다음과 같이 다양한 관계를 이해할 수 있는지 테스트합니다.  \n",
    "640 차원 단어 벡터를 사용하여 skip-gram 훈련 모델은 55 %의 의미 정확도와 59 %의 구문 정확도를 달성했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 벡터 학습\n",
    "Mikolov et al. 단어의 연속적인 벡터 표현을 처음 사용하는 것은 아니었지만 그러한 표현을 학습 할 때 계산상의 복잡성을 줄이는 방법을 보여 주었기 때문에 다량의 데이터에서 고 차원 단어 벡터를 학습하는 것이 실용적이었습니다.  \n",
    "예를 들어, \"Google은 단어 벡터 교육을 위해 Google 뉴스 자료를 사용했습니다. 이 코퍼스에는 약 6B 개의 토큰이 있습니다. 우리는 어휘 크기를 1 백만 개의 가장 빈번한 단어로 제한했습니다 ... \"  \n",
    "\n",
    "신경망 언어 모델 (피드 포워드 또는 반복)의 복잡성은 비선형 숨겨진 레이어에서 비롯됩니다.\n",
    "\n",
    "Continuous Bag-of-Words 모델과 Continuous Skip-gram 모델의 두 가지 새로운 아키텍처가 제안되었습니다 .  \n",
    "먼저 CBOW (bag-of-words) 모델을 살펴 보겠습니다.  \n",
    "\n",
    "\"최근 소개 된 연속 Skip-gram 모델은 많은 수의 정확한 구문 및 의미 단어 관계를 포착하는 고품질 분산 벡터 표현을 학습하는 효율적인 방법입니다.  \n",
    "\"텍스트 위에 슬라이딩 윈도우를 상상해보십시오.  \n",
    "그것은 현재 초점이 맞춰져있는 중심 단어와 앞 4개의 단어와 함께 그것을 포함하며 그 다음에 나오는 4개의 단어를 포함합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문맥 단어는 입력 레이어를 형성합니다.  \n",
    "input one-hot vector를 가지고 있다.\n",
    "그리고 하나의 숨겨진 레이어와 출력 레이어가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 목적은 가중치와 관련하여 입력 문맥 단어가 주어지면 실제 출력 단어 (초점 단어)를 관찰 할 조건부 확률을 최대화하는 것입니다.  \n",
    "우리의 예에서 입력 ( \"an\", \"efficient\", \"method\", \"for\", \"high\", \"quality\", \"distributed\", \"vector\")을 얻으면  \n",
    "우리의 입력 벡터는 하나의 열은 가중치 행렬에 의해 입력 벡터를 곱 하면 W1 의 한개가 선택 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 C 입력 단어 벡터를 감안할 때 , 숨겨진 계층 h에 대한 활성화 함수는 단순히 W1 에서 해당 'hot'행을 합산하고 C로 나누어 평균을 취합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 층 숨겨진 층으로부터, 제 가중치 행렬 W2는 어휘 내의 각 단어에 대한 점수를 계산하기 위해 사용될 수 있으며,  \n",
    "이 softmax 단어의 사후 분포를 얻기 위해 사용될 수있다.  \n",
    "\n",
    "skip-gram 모델은 CBOW 모델의 반대입니다.   \n",
    "단일 입력 벡터로 포커스 단어로 구성되며 대상 컨텍스트 단어가 이제 출력 레이어에 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숨겨진 층의 활성화 기능은 단순히 가중치 행렬로부터 대응하는 열을 복사에 W1에 의해 출력 계층에서 하나의 C 다항식 분포를 출력 합니다.  \n",
    "교육 목표는 출력 계층의 모든 컨텍스트 단어에 대한 합계 예측 오류를 최소화하는 것입니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화  \n",
    "교육 인스턴스의 모든 단어에 대한 모든 출력 단어 벡터를 업데이트하는 것은 매우 비쌉니다.  \n",
    "\n",
    "이 문제를 해결하기 위해 직관은 교육 인스턴스별로 업데이트해야하는 출력 벡터의 수를 제한하는 것입니다.  \n",
    "이를 달성하기위한 하나의 우아한 접근 방식은 계층 적 소프트 맥스입니다.  \n",
    "또 다른 접근법은 표본 추출을 통한 접근입니다.  \n",
    "\n",
    "계층 적 softmax는 이진 트리를 사용하여 어휘의 모든 단어를 나타냅니다.   \n",
    "단어 그 자체가 나무에 남습니다.   \n",
    "각 잎에 대해 루트에서 리프까지 고유 한 경로가 있으며이 경로는 리프가 나타내는 단어의 확률을 추정하는 데 사용됩니다.   \n",
    "\"우리는 이 확률을 문제의 잎에서 뿌리 끝에서 시작하여 무작위 걸음의 확률로 정의합니다.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네거티브 샘플링은 단순히 반복마다 출력 단어 샘플을 업데이트한다는 개념입니다.  \n",
    "목표 출력 단어는 샘플에 보관되어 업데이트 되어야하며 여기에 음수 샘플로 몇 가지 (비 대상) 단어를 추가합니다.   \n",
    "\"확률 분포는 표본 추출 과정에서 필요하며 임의로 선택할 수 있습니다 ... 경험적으로 좋은 분포를 결정할 수 있습니다.\"  \n",
    "\n",
    "Mikolov et al. 트레이닝 세트에서 드문 단어와 빈번한 단어 사이의 불균형을 막기 위해 간단한 서브 샘플링 방식을 사용하십시오   \n",
    "(예 : \"in\", \"the\"및 \"a\"는 드문 단어보다 정보 가치가 낮음).   \n",
    "훈련 세트의 각 단어는 확률 P (w i)로 폐기된다. 여기서,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./images/29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f (wi)는 단어 wi의 빈도이고 t는 선택된 임계 값, 일반적으로 약 10-5이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
